{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"################  PREPARE DATA FOR SPACY STEP 1  ################\"\"\"\n",
    "import json\n",
    "import spacy\n",
    "from spacy.gold import GoldCorpus, minibatch, biluo_tags_from_offsets, tags_to_entities\n",
    "\n",
    "\n",
    "def docs_from_offsets(nlp, gold):\n",
    "    # create a sequence of docs from a sequence of text, entity-offsets pairs\n",
    "    docs = []\n",
    "    for text, entities in gold:\n",
    "        doc = nlp(text)\n",
    "        entities = entities['entities']\n",
    "        tags = biluo_tags_from_offsets(doc, entities)\n",
    "        if entities:\n",
    "            for start, end, label in entities:\n",
    "                span = doc.char_span(start, end, label=label)\n",
    "                if span:\n",
    "                    doc.ents = list(doc.ents) + [span]\n",
    "        if doc.ents:  # remove to return documents without entities too\n",
    "            docs.append((doc, tags))\n",
    "    return docs\n",
    "\n",
    "\n",
    "def docs_to_trees(docs):\n",
    "    # create spaCy JSON training data from a sequence of docs\n",
    "    doc_trees = []\n",
    "    for d, doc_tuple in enumerate(docs):\n",
    "        doc, tags = doc_tuple\n",
    "        try:\n",
    "            tags_to_entities(tags)\n",
    "        except AssertionError:\n",
    "            print('Dropping {}'.format(d))\n",
    "            continue\n",
    "        if not tags:\n",
    "            print('Dropping {}'.format(d))\n",
    "            continue\n",
    "        sentences = []\n",
    "        for s in doc.sents:\n",
    "            s_tokens = []\n",
    "            for t in s:\n",
    "                token_data = {\n",
    "                    'id': t.i,\n",
    "                    'orth': t.orth_,\n",
    "                    'tag': t.tag_,\n",
    "                    'head': t.head.i - t.i,\n",
    "                    'dep': t.dep_,\n",
    "                    'ner': tags[t.i],\n",
    "                }\n",
    "                s_tokens.append(token_data)\n",
    "            sentences.append({'tokens': s_tokens})\n",
    "        doc_trees.append({\n",
    "            'id': d,\n",
    "            'paragraphs': [\n",
    "                {\n",
    "                    'raw': doc.text,\n",
    "                    'sentences': sentences,\n",
    "                }\n",
    "            ]\n",
    "        })\n",
    "    return doc_trees\n",
    "\n",
    "\n",
    "def train_data_to_json(train_data, nlp, output_json='train.json'):\n",
    "\n",
    "    offsets = train_data\n",
    "\n",
    "    docs = docs_from_offsets(nlp, offsets)\n",
    "\n",
    "    trees = docs_to_trees(docs)\n",
    "\n",
    "    with open(output_json, 'wt') as f:\n",
    "        json.dump(trees, f)\n",
    "    print ('successfully converted to json:',  output_json)\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "\"\"\"################  PREPARE DATA FOR SPACY STEP 2 ################\"\"\"\n",
    "\n",
    "# load pretrained spacy model and disable disturbing pipeline components:\n",
    "nlp = spacy.load('de_core_news_sm', disable=[\"tagger\", \"ner\"])\n",
    "\n",
    "\n",
    "noNER = 0\n",
    "for root, dirs, files in os.walk('/home/mareike/final_test_data'):\n",
    "    for name in files:\n",
    "        # read output json file from webanno:\n",
    "        with open('/home/mareike/final_test_data/'+name) as data_file:    \n",
    "            data = json.load(data_file)\n",
    "    \n",
    "        try:\n",
    "            # extract entity start/end positions and names:\n",
    "            ent_loc = data['_views']['_InitialView']['NamedEntity']\n",
    "        except KeyError:\n",
    "            noNER += 1\n",
    "            continue\n",
    "        \n",
    "        # extract sentence start/end positions:\n",
    "        Sentence = data['_views']['_InitialView']['Sentence']\n",
    "        \n",
    "        # set first sentence starting position 0\n",
    "        Sentence[0]['begin'] = 0\n",
    "\n",
    "        # extract original sentences:\n",
    "        sentences_list = []\n",
    "        for sent_borders in Sentence:\n",
    "            tmp_sent_string = \"\"\n",
    "            for letter in range(sent_borders[\"begin\"], sent_borders[\"end\"] ):\n",
    "                tmp_sent_string +=(data['_referenced_fss']['12']['sofaString'][letter])\n",
    "            sentences_list.append(tmp_sent_string)\n",
    "\n",
    "        # prepare spacy formatted training data:\n",
    "        TRAIN_DATA = []\n",
    "        ent_list = []\n",
    "\n",
    "        \n",
    "        for sl in range(len(Sentence)):\n",
    "            ent_list_sen = []\n",
    "            for el in range(len(ent_loc)):\n",
    "                try:\n",
    "                    if(ent_loc[el]['begin'] >= Sentence[sl]['begin'] and ent_loc[el]['end'] <= Sentence[sl]['end']):\n",
    "                        ## Need to subtract entity location with sentence begining as webanno generate data by treating document as a whole\n",
    "                        ent_list_sen.append([(ent_loc[el]['begin']-Sentence[sl]['begin']),(ent_loc[el]['end']-Sentence[sl]['begin']),ent_loc[el]['value']])\n",
    "                except KeyError:\n",
    "                    ent_loc[el]['begin'] = 0\n",
    "                    \n",
    "                    if(ent_loc[el]['begin'] >= Sentence[sl]['begin'] and ent_loc[el]['end'] <= Sentence[sl]['end']):\n",
    "                        ## Need to subtract entity location with sentence begining as webanno generate data by treating document as a whole\n",
    "                        ent_list_sen.append([(ent_loc[el]['begin']-Sentence[sl]['begin']),(ent_loc[el]['end']-Sentence[sl]['begin']),ent_loc[el]['value']])   \n",
    "                        \n",
    "            ent_list.append(ent_list_sen)\n",
    "            ## Create blank dictionary\n",
    "            ent_dic = {}\n",
    "            ## Fill value to the dictionary\n",
    "            ent_dic['entities'] = ent_list[-1]\n",
    "            ## Prepare final training data\n",
    "            TRAIN_DATA.append([sentences_list[sl],ent_dic])\n",
    "        try:    \n",
    "            train_data_to_json(TRAIN_DATA, nlp,'/home/mareike/final_test_data/spacy_json/'+name)\n",
    "        except ValueError:\n",
    "            print(name)\n",
    "            continue\n",
    "\n",
    "print('Amount of .json data without named entitys:' +str(noNER) )\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Sources:\n",
    "    https://www.thinkinfi.com/2020/01/train-custom-ner-spacy.html (modified)\n",
    "    https://support.prodi.gy/t/prodigy-annotations-to-spacy-train/243/11 (modified)\n",
    "    \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
